{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422e92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Products table loaded successfully.\n",
      "✅ Stores table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ✅ إعداد الاتصال بـ SQL Server\n",
    "server = 'localhost'\n",
    "database = 'Retail_Inventory_DB'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "connection_string = f\"DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection=yes;\"\n",
    "params = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "# ✅ إنشاء المحرك\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ تحميل جدول المنتجات (Products)\n",
    "# ==============================\n",
    "\n",
    "try:\n",
    "    df_products = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\products.csv')  # استخدم r'' لتجنب مشاكل الـ backslash\n",
    "    df_products.rename(columns={'Product ID': 'Product_ID'}, inplace=True)\n",
    "    \n",
    "    df_products.to_sql('Products', con=engine, if_exists='append', index=False)\n",
    "    print(\"✅ Products table loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error loading Products:\", e)\n",
    "\n",
    "# ==============================\n",
    "# ✅ تحميل جدول المتاجر (Stores)\n",
    "# ==============================\n",
    "\n",
    "try:\n",
    "    df_stores = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\store.csv')\n",
    "    df_stores.rename(columns={'Store ID': 'Store_ID'}, inplace=True)\n",
    "\n",
    "    df_stores.to_sql('Stores', con=engine, if_exists='append', index=False)\n",
    "    print(\"✅ Stores table loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error loading Stores:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4779861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dates table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df_dates = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\dates.csv')   \n",
    "df_dates.rename(columns={'Holiday/Promotion': 'Holiday_Promotion'}, inplace=True)\n",
    "\n",
    "try:\n",
    "    df_dates.to_sql('Dates', con=engine, if_exists='append', index=False)\n",
    "    print(\"✅ Dates table loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error loading dates:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "133e0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sales table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df_sales = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\sales.csv')\n",
    "\n",
    "# ✅ Rename columns to match DB table\n",
    "df_sales.rename(columns={\n",
    "    'Store ID': 'Store_ID',\n",
    "    'Product ID': 'Product_ID',\n",
    "    'Units Sold': 'Units_Sold',\n",
    "    'Units Ordered': 'Units_Ordered'\n",
    "}, inplace=True)\n",
    "\n",
    "# ✅ Load mapping from Stores table\n",
    "store_map = pd.read_sql('SELECT Store_Region_ID, Store_ID FROM Stores', con=engine)\n",
    "store_dict = dict(zip(store_map['Store_ID'], store_map['Store_Region_ID']))\n",
    "\n",
    "# ✅ Load mapping from Products table\n",
    "product_map = pd.read_sql('SELECT Product_Entry_ID, Product_ID FROM Products', con=engine)\n",
    "product_dict = dict(zip(product_map['Product_ID'], product_map['Product_Entry_ID']))\n",
    "\n",
    "# ✅ Map string IDs to surrogate keys\n",
    "df_sales['Store_ID'] = df_sales['Store_ID'].map(store_dict)\n",
    "df_sales['Product_ID'] = df_sales['Product_ID'].map(product_dict)\n",
    "\n",
    "# ✅ Check for unmapped rows\n",
    "missing_store_ids = df_sales[df_sales['Store_ID'].isna()]\n",
    "missing_product_ids = df_sales[df_sales['Product_ID'].isna()]\n",
    "\n",
    "if not missing_store_ids.empty or not missing_product_ids.empty:\n",
    "    print(\"❌ There are unmapped IDs:\")\n",
    "    if not missing_store_ids.empty:\n",
    "        print(\"Missing Store_IDs:\\n\", missing_store_ids[['Date', 'Store_ID']].drop_duplicates())\n",
    "    if not missing_product_ids.empty:\n",
    "        print(\"Missing Product_IDs:\\n\", missing_product_ids[['Date', 'Product_ID']].drop_duplicates())\n",
    "else:\n",
    "    # ✅ Load to SQL if no missing keys\n",
    "    try:\n",
    "        df_sales.to_sql('Sales', con=engine, if_exists='append', index=False)\n",
    "        print(\"✅ Sales table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error loading Sales:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6ecccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Promotions table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ✅ قراءة بيانات العروض\n",
    "df_promotions = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\promotion.csv')\n",
    "\n",
    "# ✅ تعديل أسماء الأعمدة لتتناسب مع قاعدة البيانات\n",
    "df_promotions.rename(columns={\n",
    "    'Store ID': 'Store_ID',\n",
    "    'Product ID': 'Product_ID',\n",
    "    'Holiday/Promotion': 'Holiday_Promotion'\n",
    "}, inplace=True)\n",
    "\n",
    "# ✅ تحميل المفاتيح البديلة من قاعدة البيانات\n",
    "store_map = pd.read_sql('SELECT Store_Region_ID, Store_ID FROM Stores', con=engine)\n",
    "store_dict = dict(zip(store_map['Store_ID'], store_map['Store_Region_ID']))\n",
    "\n",
    "product_map = pd.read_sql('SELECT Product_Entry_ID, Product_ID FROM Products', con=engine)\n",
    "product_dict = dict(zip(product_map['Product_ID'], product_map['Product_Entry_ID']))\n",
    "\n",
    "# ✅ تحويل الـ Store_ID و Product_ID إلى المفاتيح البديلة\n",
    "df_promotions['Store_ID'] = df_promotions['Store_ID'].map(store_dict)\n",
    "df_promotions['Product_ID'] = df_promotions['Product_ID'].map(product_dict)\n",
    "\n",
    "# ✅ التحقق من القيم المفقودة (IDs غير مرتبطة)\n",
    "missing_store_ids = df_promotions[df_promotions['Store_ID'].isna()]\n",
    "missing_product_ids = df_promotions[df_promotions['Product_ID'].isna()]\n",
    "\n",
    "if not missing_store_ids.empty or not missing_product_ids.empty:\n",
    "    print(\"❌ There are unmapped IDs in Promotions:\")\n",
    "    if not missing_store_ids.empty:\n",
    "        print(\"Missing Store_IDs:\\n\", missing_store_ids[['Date', 'Store_ID']].drop_duplicates())\n",
    "    if not missing_product_ids.empty:\n",
    "        print(\"Missing Product_IDs:\\n\", missing_product_ids[['Date', 'Product_ID']].drop_duplicates())\n",
    "else:\n",
    "    # ✅ تحميل البيانات إلى قاعدة البيانات\n",
    "    try:\n",
    "        df_promotions.to_sql('Promotions', con=engine, if_exists='append', index=False)\n",
    "        print(\"✅ Promotions table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error loading Promotions:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cd1416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inventory table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ✅ قراءة بيانات الجرد (Inventory)\n",
    "df_inventory = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\inventory.csv')\n",
    "\n",
    "# ✅ تعديل أسماء الأعمدة لتتناسب مع قاعدة البيانات\n",
    "df_inventory.rename(columns={\n",
    "    'Store ID': 'Store_ID',\n",
    "    'Product ID': 'Product_ID',\n",
    "    'Inventory Level': 'Inventory_Level'\n",
    "}, inplace=True)\n",
    "\n",
    "# ✅ تحميل المفاتيح البديلة من قاعدة البيانات\n",
    "store_map = pd.read_sql('SELECT Store_Region_ID, Store_ID FROM Stores', con=engine)\n",
    "store_dict = dict(zip(store_map['Store_ID'], store_map['Store_Region_ID']))\n",
    "\n",
    "product_map = pd.read_sql('SELECT Product_Entry_ID, Product_ID FROM Products', con=engine)\n",
    "product_dict = dict(zip(product_map['Product_ID'], product_map['Product_Entry_ID']))\n",
    "\n",
    "# ✅ تحويل Store_ID و Product_ID للمفاتيح البديلة\n",
    "df_inventory['Store_ID'] = df_inventory['Store_ID'].map(store_dict)\n",
    "df_inventory['Product_ID'] = df_inventory['Product_ID'].map(product_dict)\n",
    "\n",
    "# ✅ التحقق من القيم المفقودة\n",
    "missing_store_ids = df_inventory[df_inventory['Store_ID'].isna()]\n",
    "missing_product_ids = df_inventory[df_inventory['Product_ID'].isna()]\n",
    "\n",
    "if not missing_store_ids.empty or not missing_product_ids.empty:\n",
    "    print(\"❌ There are unmapped IDs in Inventory:\")\n",
    "    if not missing_store_ids.empty:\n",
    "        print(\"Missing Store_IDs:\\n\", missing_store_ids[['Date', 'Store_ID']].drop_duplicates())\n",
    "    if not missing_product_ids.empty:\n",
    "        print(\"Missing Product_IDs:\\n\", missing_product_ids[['Date', 'Product_ID']].drop_duplicates())\n",
    "else:\n",
    "    # ✅ تحميل البيانات إلى SQL\n",
    "    try:\n",
    "        df_inventory.to_sql('Inventory', con=engine, if_exists='append', index=False)\n",
    "        print(\"✅ Inventory table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error loading Inventory:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5bd885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forecast table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ✅ قراءة بيانات التنبؤ بالطلب (Forecast)\n",
    "df_forecast = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\forecast.csv')\n",
    "\n",
    "# ✅ تعديل أسماء الأعمدة لتتناسب مع قاعدة البيانات\n",
    "df_forecast.rename(columns={\n",
    "    'Store ID': 'Store_ID',\n",
    "    'Product ID': 'Product_ID',\n",
    "    'Demand Forecast': 'Demand_Forecast'\n",
    "}, inplace=True)\n",
    "\n",
    "# ✅ تحميل المفاتيح البديلة من SQL\n",
    "store_map = pd.read_sql('SELECT Store_Region_ID, Store_ID FROM Stores', con=engine)\n",
    "store_dict = dict(zip(store_map['Store_ID'], store_map['Store_Region_ID']))\n",
    "\n",
    "product_map = pd.read_sql('SELECT Product_Entry_ID, Product_ID FROM Products', con=engine)\n",
    "product_dict = dict(zip(product_map['Product_ID'], product_map['Product_Entry_ID']))\n",
    "\n",
    "# ✅ ربط الـ IDs بالمفاتيح البديلة\n",
    "df_forecast['Store_ID'] = df_forecast['Store_ID'].map(store_dict)\n",
    "df_forecast['Product_ID'] = df_forecast['Product_ID'].map(product_dict)\n",
    "\n",
    "# ✅ التحقق من القيم غير المرتبطة\n",
    "missing_store_ids = df_forecast[df_forecast['Store_ID'].isna()]\n",
    "missing_product_ids = df_forecast[df_forecast['Product_ID'].isna()]\n",
    "\n",
    "if not missing_store_ids.empty or not missing_product_ids.empty:\n",
    "    print(\"❌ There are unmapped IDs in Forecast:\")\n",
    "    if not missing_store_ids.empty:\n",
    "        print(\"Missing Store_IDs:\\n\", missing_store_ids[['Date', 'Store_ID']].drop_duplicates())\n",
    "    if not missing_product_ids.empty:\n",
    "        print(\"Missing Product_IDs:\\n\", missing_product_ids[['Date', 'Product_ID']].drop_duplicates())\n",
    "else:\n",
    "    # ✅ تحميل البيانات إلى SQL\n",
    "    try:\n",
    "        df_forecast.to_sql('Forecast', con=engine, if_exists='append', index=False)\n",
    "        print(\"✅ Forecast table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error loading Forecast:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0d370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحميل بيانات Competitor بنجاح.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# تحميل بيانات المنافسين من ملف CSV\n",
    "df_competitor = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\competitor.csv')\n",
    "\n",
    "# تعديل أسماء الأعمدة لتتناسب مع قاعدة البيانات\n",
    "df_competitor.rename(columns={\n",
    "    'Product ID': 'Product_ID',\n",
    "    'Competitor Pricing': 'Competitor_Pricing'\n",
    "}, inplace=True)\n",
    "\n",
    "# تحميل المفاتيح البديلة من SQL للمنتجات\n",
    "product_map = pd.read_sql('SELECT Product_Entry_ID, Product_ID FROM Products', con=engine)\n",
    "product_dict = dict(zip(product_map['Product_ID'], product_map['Product_Entry_ID']))\n",
    "\n",
    "# ربط Product_ID بالمفتاح البديل Product_Entry_ID\n",
    "df_competitor['Product_ID'] = df_competitor['Product_ID'].map(product_dict)\n",
    "\n",
    "# التحقق من القيم غير المرتبطة\n",
    "missing_product_ids = df_competitor[df_competitor['Product_ID'].isna()]\n",
    "\n",
    "if not missing_product_ids.empty:\n",
    "    print(\"❌ هناك Product_IDs غير مرتبطة في Competitor:\")\n",
    "    print(missing_product_ids[['Date', 'Product_ID']].drop_duplicates())\n",
    "else:\n",
    "    # تحميل البيانات إلى جدول Competitor في قاعدة البيانات\n",
    "    try:\n",
    "        df_competitor.to_sql('Competitor', con=engine, if_exists='append', index=False)\n",
    "        print(\"✅ تم تحميل بيانات Competitor بنجاح.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ خطأ أثناء تحميل بيانات Competitor:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "355080be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Weather table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# تحميل بيانات الطقس من ملف CSV\n",
    "df_weather = pd.read_csv(r'C:\\Users\\Hanin Baher\\OneDrive\\Documents\\Retail\\datasets\\weather.csv')\n",
    "\n",
    "# تعديل أسماء الأعمدة لتتناسب مع قاعدة البيانات\n",
    "df_weather.rename(columns={\n",
    "    'Weather Condition': 'Weather_Condition'\n",
    "}, inplace=True)\n",
    "\n",
    "# تحميل البيانات إلى جدول Weather في قاعدة البيانات\n",
    "try:\n",
    "    df_weather.to_sql('Weather', con=engine, if_exists='append', index=False)\n",
    "    print(\"✅ Weather table loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error loading Weather:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
